
#### 1 Machine Learning for Predictive Data Analysis 
- 1.1. What is predictive Data Analytics? p1
- 1.2. What is Machine Learning? p3
- 1.3. How does Machine Learning work? p5
- 1.4. What can go wrong with Machine Learning? p11
- 1.5. The Predictive Data Analytics Project Lifecycle: CRISP-DM p12
- 1.6. Predictive Data Analytics Tools p15
- 1.7. The Road Ahead p17
- 1.8. Exercises p19

#### 2 Data to Insights to Decisions p21
- 2.1. Converting Business Problems into Analytic Solutions p21
  - 2.1.1. Case Study: Motor Insurance Fraud p23
- 2.2. Assessing Feasability p24
  - 2.2.1. Case Study: Motor Insurance Fraud p26
- 2.3. Designing the Analytic Base Table p27
  - 2.3.1. Case Study: Motor Insurance Problem p31
- 2.4. Designing and Implementing Features p32
  - 2.4.1. Different Types and Data p34
  - 2.4.2. Different Types of Features p34
  - 2.4.3. Handling Time p37
  - 2.4.4. Legal Issues p41
  - 2.4.5. Implementing Features p43
  - 2.4.6. Case Study: Motor Insurance Fraud p43
- 2.5. Summary p46
- 2.6. Further Reading p49
- 2.7. Exercises p50

#### 3 Data Exploration p55
- 3.1. The Data Quality Report p56
  - 3.1.1. Case Study: Motor Insurance Fraud p57
- 3.2. Getting to know the data p61
  - 3.2.1. The Normal Distribution p64
  - 3.2.2. Case Study: Motor Insurance Fraud p65
- 3.3. Identifying Data Quality Issues p66
  - 3.3.1. Missing Values p67
  - 3.3.2. Irregular Cardinality p68
  - 3.3.3. Outliers p68
  - 3.3.4. Case Study: Motor Insurance Fraud p70
- 3.4. Handling Data Quality Issues p73
- 3.5. Advanced Data Exploration p77
  - 3.5.1. Visualizing Relationships Between Features p77
  - 3.5.2. Measuring Covariance and Correlation p86
- 3.6. Data Preparation p92
  - 3.6.1. Normalization p92
  - 3.6.2. Binning p94
  - 3.6.3. Sampling p98
- 3.7. Summary p100
- 3.8. Further Reading p101
- 3.9. Exercises p103

#### 4 Information-Based Learning p117
- 4.1. Big Idea p117
- 4.2. Fundamentals p120
  - 4.2.1. Decision Trees p121
  - 4.2.2. Shannon's Entropy Model p124
  - 4.2.3. Information Gain p128
- 4.3. Standard Approach: The ID3 Algorithm p134
  - 4.3.1. A Worked Example: Predicting Vegatation Distributions p137
- 4.4. Extensions and Variations p144
  - 4.4.1. Alternative Feature Selection and Impurity Metrics p144
  - 4.4.2. Handling Continuous Descriptive Features p150
  - 4.4.3. Predicting Continuous Targets p153
  - 4.4.4. Tree Pruning p158
  - 4.4.5. Model Ensembles p163
- 4.5. Summary p167
- 4.6. Further Reading p169
- 4.7. Exercises p170

#### 5 Similarity-based Learning p179
- 5.1. Big Idea p170
- 5.2. Fundamentals p180
  - 5.2.1. Feature Space p181
  - 5.2.2. Measuring Similarity Using Distance Metrics p183
- 5.3. Standard Approach: The Nearest Neighbor Algorithm p186
  - 5.3.1. A Worked Example p186
- 5.4. Extensions and Variations p190
  - 5.4.1. Handling Noisy Data p190
  - 5.4.2. Efficient Memory Search p195
  - 5.4.3. Data Normalization p204
  - 5.4.4. Predicting Continuous Targets p209
  - 5.4.5. Other Measures of Similarity p212
  - 5.4.6. Feature Selection p226
- 5.5. Summary 234
- 5.6. Further Reading p237
- 5.7. Epilogue p238
- 5.8. Exercises p240

#### 6 Probability-based Learning p247
- 6.1. Big Idea p247
- 6.2. Fundamentals p249
  - 6.2.1. Bayes' Theorem p252
  - 6.2.2. Bayesian Prediction p256
  - 6.2.3. Conditional Independence and Factorization p262
- 6.3. Standard Approach: The Naive Bayes Model p267
  - 6.3.1. A Worked Example p269
- 6.4. Extensions and Variations p272
  - 6.4.1. Smoothing p272
  - 6.4.2. Continuous Features: Probability Density Functions p276
  - 6.4.3. Continuous Features: Binning p289
  - 6.4.4. Bayesian Networks p292
- 6.5. Summary p312
- 6.6. Further Reading p315
- 6.7. Exercises p317

#### 7 Error-based Learning 323
- 7.1. Big Idea p323
- 7.2. Fundamentals p324
  - 7.2.1. Simple Linear Regression p324
  - 7.2.2. Measuring Error p327
  - 7.2.3. Error Surfaces p330 
- 7.3. Standard Approach: Multivariate Linear Regression with Gradient Descent p332
  - 7.3.1. Multivariable Linear Regression p332
  - 7.3.2. Gradient Descent p334
  - 7.3.3. Choosing Learning Weights and Initial Weights p341
  - 7.3.4. A Worked Example p343
- 7.4. Extensions and Variations p346
  - 7.4.1. Interpreting Multivariable Linear Regression Models p347
  - 7.4.2. Setting the Learning Rate Using Weight Decay p349
  - 7.4.3. Handling Categorical Descriptive Features p351
  - 7.4.4. Handling Categorical Target Features: Logistic Regression p353
  - 7.4.5. Modeling Non-linear Relationships p365
  - 7.4.6. Multinomial Logistic Regression p373
  - 7.4.7. Support Vector Machines p376
- 7.5. Summary p383
- 7.6. Further Reading p386
- 7.7. Exercises p388

#### 8 Evaluation 397
- 8.1. Big Idea p397
- 8.2. Fundamentals p398
- 8.3. Standard Approach: Misclassification Rate on Hold-out Test Set p399
- 8.4. Extensions and Variations p405
  - 8.4.1. Designing Evaluation Experiments p405
  - 8.4.2. Performance Measures: Categorical Targets p413
  - 8.4.3. Performance Measures: Prediction Scores p423
  - 8.4.4. Performance Measures: Multinomial Targets p440
  - 8.4.5. Performance Measures: Continuous Targets p442
  - 8.4.6. Everday Models after Deployment p447
- 8.5. Summary p455
- 8.6. Further Reading p456
- 8.7. Exercises p457

#### 9 Case Study: Customer Churn p463
- 9.1. Business Understanding p463
- 9.2. Data Understanding p467
- 9.3. Data Preparation p471
- 9.4. Modeling p477
- 9.5. Evaluation p479
- 9.6. Deployment p482

#### 10 Case Study: Galaxy Classification p483
- 10.1. Business Understanding p483
  - 10.1.1. Situational Fluency p486
- 10.2. Data Understanding p488
- 10.3. Data Preparation p495
- 10.4. Modeling p500
  - 10.4.1. Baseline Models p500
  - 10.4.2. Feature Selection p503
  - 10.4.3. The 5-level Model p505
- 10.5. Evaluation p508
- 10.6. Deployment p509

#### 11 The Art of Machine Learning for Predictive Data Analytics p511
- 11.1. Differential Perspectives on Prediction Models p513
- 11.2. Choosing a Machine Learning Approach p518
  - 11.2.1. Matching Machine Learning Approaches to Projects p521
  - 11.2.2. Matching Machine Learning Approaches to Data p522
- 11.3. Your Next Steps p523

#### A Descriptive Statistics and Data Visualization for Machine Learning p525
- A.1. Descriptive Statistics for Continuous Features p525
  - A.1.1. Central Tendency p525
  - A.1.2. Variation p527
- A.2. Descriptive Statistics for Categorical Features p530
- A.3. Populations and Samples p532
- A.4. Data Visualization p534
  - A 4.1. Bar Plots p534
  - A 4.2. Histograms p534
  - A 4.3. Box Plots p538

#### B Introduction to Probability for Machine Learning p541
- B.1. Probability Basics p541
- B.2. Probability Distributions and Summing Out p546
- B.3. Some Useful Probability Rules p548
- B.4. Summary p550

#### C Differentiation Techniques for Machine Learning p551
- C.1. Derivatives of Continuous Functions p552
- C.2. The Chain Rule p554
- C.3. Partial Derivatives p555
    